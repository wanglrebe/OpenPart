# backend/quick_db_setup.py
"""
å¿«é€Ÿæ•°æ®åº“è®¾ç½®è„šæœ¬ - åˆ›å»ºæ’ä»¶ç®¡ç†å¿…éœ€çš„è¡¨
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from sqlalchemy import create_engine, text
from app.core.config import settings

def create_plugin_tables():
    """åˆ›å»ºæ’ä»¶ç›¸å…³è¡¨"""
    print("ğŸš€ åˆ›å»ºæ’ä»¶ç®¡ç†æ•°æ®åº“è¡¨...")
    
    try:
        engine = create_engine(settings.database_url)
        
        with engine.connect() as conn:
            # æµ‹è¯•è¿æ¥
            conn.execute(text("SELECT 1"))
            print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
            
            # åˆ›å»ºæ’ä»¶è¡¨
            plugin_table_sql = """
                CREATE TABLE IF NOT EXISTS crawler_plugins (
                    id SERIAL PRIMARY KEY,
                    name VARCHAR(100) UNIQUE NOT NULL,
                    display_name VARCHAR(200) NOT NULL,
                    version VARCHAR(50) NOT NULL,
                    description TEXT,
                    author VARCHAR(100),
                    data_source VARCHAR(200),
                    file_path VARCHAR(500) NOT NULL,
                    status VARCHAR(20) DEFAULT 'inactive',
                    is_active BOOLEAN DEFAULT FALSE,
                    config JSON,
                    run_count INTEGER DEFAULT 0,
                    success_count INTEGER DEFAULT 0,
                    error_count INTEGER DEFAULT 0,
                    last_run_at TIMESTAMP,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """
            conn.execute(text(plugin_table_sql))
            print("âœ… æ’ä»¶è¡¨åˆ›å»ºæˆåŠŸ")
            
            # åˆ›å»ºä»»åŠ¡è¡¨
            task_table_sql = """
                CREATE TABLE IF NOT EXISTS crawler_tasks (
                    id SERIAL PRIMARY KEY,
                    plugin_id INTEGER REFERENCES crawler_plugins(id) ON DELETE CASCADE,
                    name VARCHAR(200) NOT NULL,
                    description TEXT,
                    config JSON,
                    status VARCHAR(20) DEFAULT 'pending',
                    run_count INTEGER DEFAULT 0,
                    started_at TIMESTAMP,
                    finished_at TIMESTAMP,
                    execution_time FLOAT,
                    data_count INTEGER DEFAULT 0,
                    logs JSON,
                    error_message TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """
            conn.execute(text(task_table_sql))
            print("âœ… ä»»åŠ¡è¡¨åˆ›å»ºæˆåŠŸ")
            
            # åˆ›å»ºè¿ç§»è®°å½•è¡¨
            migration_table_sql = """
                CREATE TABLE IF NOT EXISTS schema_migrations (
                    version VARCHAR(50) PRIMARY KEY,
                    applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """
            conn.execute(text(migration_table_sql))
            print("âœ… è¿ç§»è®°å½•è¡¨åˆ›å»ºæˆåŠŸ")
            
            # è®°å½•è¿ç§»ç‰ˆæœ¬
            conn.execute(text("""
                INSERT INTO schema_migrations (version) 
                VALUES ('002_plugin_tables') 
                ON CONFLICT (version) DO NOTHING
            """))
            
            # ä¸ºé›¶ä»¶è¡¨æ·»åŠ æ•°æ®æºå­—æ®µï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            try:
                # æ£€æŸ¥å­—æ®µæ˜¯å¦å­˜åœ¨
                result = conn.execute(text("""
                    SELECT column_name 
                    FROM information_schema.columns 
                    WHERE table_name = 'parts' AND column_name = 'data_source'
                """))
                
                if not result.fetchone():
                    conn.execute(text("""
                        ALTER TABLE parts 
                        ADD COLUMN data_source VARCHAR(200),
                        ADD COLUMN source_url VARCHAR(500),
                        ADD COLUMN crawl_time TIMESTAMP
                    """))
                    print("âœ… é›¶ä»¶è¡¨å­—æ®µæ·»åŠ æˆåŠŸ")
                else:
                    print("âœ… é›¶ä»¶è¡¨å­—æ®µå·²å­˜åœ¨")
                    
            except Exception as e:
                print(f"âš ï¸ é›¶ä»¶è¡¨å­—æ®µæ›´æ–°: {e}")
            
            # åˆ›å»ºç´¢å¼•
            try:
                conn.execute(text("CREATE INDEX IF NOT EXISTS idx_crawler_plugins_name ON crawler_plugins(name)"))
                conn.execute(text("CREATE INDEX IF NOT EXISTS idx_crawler_tasks_plugin_id ON crawler_tasks(plugin_id)"))
                print("âœ… ç´¢å¼•åˆ›å»ºæˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ ç´¢å¼•åˆ›å»º: {e}")
            
            # æäº¤æ‰€æœ‰æ›´æ”¹
            conn.commit()
            
            # éªŒè¯è¡¨æ˜¯å¦åˆ›å»ºæˆåŠŸ
            result = conn.execute(text("""
                SELECT table_name FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_name IN ('crawler_plugins', 'crawler_tasks', 'schema_migrations')
                ORDER BY table_name
            """))
            
            created_tables = [row[0] for row in result.fetchall()]
            print(f"ğŸ“‹ å·²åˆ›å»ºçš„è¡¨: {created_tables}")
            
            if len(created_tables) == 3:
                print("ğŸ‰ æ‰€æœ‰è¡¨åˆ›å»ºå®Œæˆï¼")
                return True
            else:
                print(f"âš ï¸ æœŸæœ›3ä¸ªè¡¨ï¼Œå®é™…åˆ›å»ºäº†{len(created_tables)}ä¸ª")
                return False
            
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è®¾ç½®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def check_tables():
    """æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨"""
    print("ğŸ” æ£€æŸ¥æ•°æ®åº“è¡¨...")
    
    try:
        engine = create_engine(settings.database_url)
        
        with engine.connect() as conn:
            result = conn.execute(text("""
                SELECT table_name FROM information_schema.tables 
                WHERE table_schema = 'public'
                ORDER BY table_name
            """))
            
            all_tables = [row[0] for row in result.fetchall()]
            print(f"ğŸ“‹ æ•°æ®åº“ä¸­çš„æ‰€æœ‰è¡¨: {all_tables}")
            
            required_tables = ['crawler_plugins', 'crawler_tasks', 'schema_migrations']
            missing_tables = [t for t in required_tables if t not in all_tables]
            
            if missing_tables:
                print(f"âŒ ç¼ºå°‘è¡¨: {missing_tables}")
                return False
            else:
                print("âœ… æ‰€æœ‰å¿…éœ€è¡¨éƒ½å­˜åœ¨")
                return True
                
    except Exception as e:
        print(f"âŒ æ£€æŸ¥å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("=" * 50)
    
    # å…ˆæ£€æŸ¥ç°çŠ¶
    if check_tables():
        print("âœ… æ•°æ®åº“è¡¨å·²å­˜åœ¨ï¼Œæ— éœ€åˆ›å»º")
    else:
        print("ğŸ”„ å¼€å§‹åˆ›å»ºæ•°æ®åº“è¡¨...")
        if create_plugin_tables():
            print("\nğŸ‰ æ•°æ®åº“è®¾ç½®å®Œæˆï¼")
            print("âœ… å¯ä»¥å¯åŠ¨æœåŠ¡å™¨äº†ï¼")
            print("å¯åŠ¨å‘½ä»¤: uvicorn app.main:app --reload --host 0.0.0.0 --port 8000")
        else:
            print("\nâŒ æ•°æ®åº“è®¾ç½®å¤±è´¥ï¼Œè¯·æ£€æŸ¥:")
            print("1. æ•°æ®åº“è¿æ¥é…ç½®")
            print("2. æ•°æ®åº“ç”¨æˆ·æƒé™")
            print("3. PostgreSQLæœåŠ¡æ˜¯å¦è¿è¡Œ")
            sys.exit(1)
    
    print("=" * 50)