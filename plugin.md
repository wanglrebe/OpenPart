# OpenPart çˆ¬è™«æ’ä»¶å¼€å‘è€…æŒ‡å—

## ğŸ“– ç›®å½•

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
3. [æ’ä»¶æ¶æ„å’Œè§„èŒƒ](#æ’ä»¶æ¶æ„å’Œè§„èŒƒ)
4. [é…ç½®ç³»ç»Ÿè¯¦è§£](#é…ç½®ç³»ç»Ÿè¯¦è§£)
5. [å®‰å…¨çº¦æŸå’Œè§„èŒƒ](#å®‰å…¨çº¦æŸå’Œè§„èŒƒ)
6. [æ•°æ®æ¨¡å‹å’Œç±»å‹](#æ•°æ®æ¨¡å‹å’Œç±»å‹)
7. [APIæ¥å£è§„èŒƒ](#apiæ¥å£è§„èŒƒ)
8. [å›¾ç‰‡ä¸‹è½½æ¥å£](#å›¾ç‰‡ä¸‹è½½æ¥å£)
9. [æ–‡ä»¶ä¸Šä¼ å’Œæ‰¹é‡å¤„ç†æ¥å£](#æ–‡ä»¶ä¸Šä¼ å’Œæ‰¹é‡å¤„ç†æ¥å£)
10. [å®Œæ•´ç¤ºä¾‹](#å®Œæ•´ç¤ºä¾‹)
11. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
12. [è°ƒè¯•å’Œæµ‹è¯•](#è°ƒè¯•å’Œæµ‹è¯•)
13. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## æ¦‚è¿°

OpenPart çˆ¬è™«æ’ä»¶ç³»ç»Ÿæ˜¯ä¸€ä¸ªå®‰å…¨ã€çµæ´»ã€æ˜“æ‰©å±•çš„æ•°æ®é‡‡é›†æ¡†æ¶ï¼Œå…è®¸å¼€å‘è€…åˆ›å»ºè‡ªå®šä¹‰çš„é›¶ä»¶æ•°æ®çˆ¬è™«æ’ä»¶ã€‚ç³»ç»Ÿæä¾›äº†å®Œæ•´çš„æ•°æ®é‡‡é›†ã€å¤„ç†å’Œå­˜å‚¨èƒ½åŠ›ã€‚

### ğŸ¯ ä¸»è¦ç‰¹æ€§

- **ç±»å‹å®‰å…¨**ï¼šåŸºäº Pydantic çš„ä¸¥æ ¼ç±»å‹æ£€æŸ¥
- **å®‰å…¨æ²™ç®±**ï¼šAST é™æ€åˆ†æé˜²æ­¢æ¶æ„ä»£ç æ‰§è¡Œ
- **åŠ¨æ€é…ç½®**ï¼šå¯è§†åŒ–çš„æ’ä»¶é…ç½®ç•Œé¢
- **ä»»åŠ¡è°ƒåº¦**ï¼šæ”¯æŒæ‰‹åŠ¨ã€å®šæ—¶å’Œé—´éš”æ‰§è¡Œ
- **å®æ—¶ç›‘æ§**ï¼šå®Œæ•´çš„æ—¥å¿—è®°å½•å’ŒçŠ¶æ€è·Ÿè¸ª
- **å›¾ç‰‡ä¸‹è½½**ï¼šå®‰å…¨çš„è¿œç¨‹å›¾ç‰‡ä¸‹è½½å’Œæœ¬åœ°å­˜å‚¨
- **æ–‡ä»¶å¤„ç†**ï¼šæ”¯æŒPDFã€Excelç­‰æ–‡æ¡£è§£æå’Œæ‰¹é‡æ•°æ®å¯¼å…¥
- **å‰ç«¯é›†æˆ**ï¼šå¼€ç®±å³ç”¨çš„ç®¡ç†ç•Œé¢

### ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```
OpenPart æ’ä»¶ç³»ç»Ÿ
â”œâ”€â”€ æ’ä»¶åŸºç±» (BaseCrawlerPlugin)
â”œâ”€â”€ æ’ä»¶ç®¡ç†å™¨ (PluginManager)
â”œâ”€â”€ å®‰å…¨éªŒè¯å™¨ (SecurityValidator)
â”œâ”€â”€ ä»»åŠ¡è°ƒåº¦å™¨ (TaskScheduler)
â”œâ”€â”€ å›¾ç‰‡ä¸‹è½½æœåŠ¡ (ImageDownloadAPI)
â”œâ”€â”€ æ–‡ä»¶å¤„ç†æœåŠ¡ (FileUploadAPI)
â”œâ”€â”€ API æ¥å£å±‚ (FastAPI)
â””â”€â”€ å‰ç«¯ç®¡ç†ç•Œé¢ (Vue 3)
```

---

## å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

**å¿…éœ€ä¾èµ–ï¼š**
```python
# ç³»ç»Ÿå·²å†…ç½®ï¼Œæ— éœ€å®‰è£…
from app.plugins.crawler_base import BaseCrawlerPlugin
from typing import Dict, Any, List
from pydantic import BaseModel
import requests  # HTTPè¯·æ±‚
import json      # JSONå¤„ç†
import time      # æ—¶é—´æ§åˆ¶
```

### 2. åˆ›å»ºåŸºç¡€æ’ä»¶

```python
# my_plugin.py
from app.plugins.crawler_base import (
    BaseCrawlerPlugin, PluginInfo, ConfigField,
    CrawlResult, TestResult, PartData, DataSourceType
)
import requests
import json

class MyPlugin(BaseCrawlerPlugin):
    """æˆ‘çš„ç¬¬ä¸€ä¸ªæ’ä»¶"""
    
    @property
    def plugin_info(self) -> PluginInfo:
        return PluginInfo(
            name="æˆ‘çš„æ’ä»¶",
            version="1.0.0",
            description="æ’ä»¶æè¿°",
            author="ä½ çš„åå­—",
            data_source="æ•°æ®æºåç§°",
            data_source_type=DataSourceType.ECOMMERCE
        )
    
    @property
    def config_schema(self) -> List[ConfigField]:
        return [
            ConfigField(
                name="api_url",
                label="APIåœ°å€",
                type="url",
                required=True,
                help_text="ç›®æ ‡ç½‘ç«™çš„APIåœ°å€"
            )
        ]
    
    def validate_config(self, config: Dict[str, Any]) -> bool:
        if not config.get("api_url"):
            raise ValueError("APIåœ°å€ä¸èƒ½ä¸ºç©º")
        return True
    
    def test_connection(self, config: Dict[str, Any]) -> TestResult:
        try:
            response = requests.get(config["api_url"], timeout=10)
            return TestResult(
                success=True,
                message="è¿æ¥æˆåŠŸ",
                response_time=1.0
            )
        except Exception as e:
            return TestResult(
                success=False,
                message=f"è¿æ¥å¤±è´¥: {str(e)}"
            )
    
    def crawl(self, config: Dict[str, Any], **kwargs) -> CrawlResult:
        # å®ç°æ•°æ®çˆ¬å–é€»è¾‘
        parts = []  # çˆ¬å–çš„é›¶ä»¶æ•°æ®
        
        return CrawlResult(
            success=True,
            data=parts,
            total_count=len(parts)
        )

# å¿…éœ€ï¼šåˆ›å»ºæ’ä»¶å®ä¾‹
plugin = MyPlugin()
```

### 3. ä¸Šä¼ å’Œæµ‹è¯•

1. å°† `.py` æ–‡ä»¶ä¸Šä¼ åˆ°ç®¡ç†åå°
2. åœ¨æ’ä»¶ç®¡ç†ç•Œé¢è¿›è¡Œé…ç½®
3. æµ‹è¯•è¿æ¥å¹¶æ‰§è¡Œçˆ¬å–ä»»åŠ¡

---

## æ’ä»¶æ¶æ„å’Œè§„èŒƒ

### æ ¸å¿ƒæ¥å£

æ‰€æœ‰æ’ä»¶å¿…é¡»ç»§æ‰¿ `BaseCrawlerPlugin` å¹¶å®ç°ä»¥ä¸‹æŠ½è±¡æ–¹æ³•ï¼š

#### 1. plugin_info å±æ€§

```python
@property
@abstractmethod
def plugin_info(self) -> PluginInfo:
    """è¿”å›æ’ä»¶åŸºæœ¬ä¿¡æ¯"""
    pass
```

**PluginInfo å­—æ®µè¯´æ˜ï¼š**

| å­—æ®µ | ç±»å‹ | å¿…éœ€ | è¯´æ˜ |
|------|------|------|------|
| name | str | âœ… | æ’ä»¶åç§°ï¼ˆä¸­æ–‡å¯è¯»ï¼‰ |
| version | str | âœ… | ç‰ˆæœ¬å·ï¼ˆå¦‚ï¼š1.0.0ï¼‰ |
| description | str | âœ… | æ’ä»¶åŠŸèƒ½æè¿° |
| author | str | âœ… | å¼€å‘è€…å§“åæˆ–å›¢é˜Ÿ |
| data_source | str | âœ… | æ•°æ®æºåç§° |
| data_source_type | DataSourceType | âœ… | æ•°æ®æºç±»å‹æšä¸¾ |
| homepage | str | âŒ | æ•°æ®æºå®˜ç½‘ |
| terms_url | str | âŒ | æœåŠ¡æ¡æ¬¾é“¾æ¥ |
| rate_limit | int | âŒ | è¯·æ±‚é¢‘ç‡é™åˆ¶(ç§’) |
| batch_size | int | âŒ | æ‰¹æ¬¡å¤„ç†å¤§å° |

**æ•°æ®æºç±»å‹æšä¸¾ï¼š**
```python
class DataSourceType(Enum):
    ECOMMERCE = "ecommerce"     # ç”µå•†å¹³å°
    SUPPLIER = "supplier"       # ä¾›åº”å•†ç½‘ç«™  
    DATABASE = "database"       # æ•°æ®åº“
    API = "api"                # APIæ¥å£
    CATALOG = "catalog"        # äº§å“ç›®å½•
    DOCUMENT = "document"      # æ–‡æ¡£å¤„ç†
    OTHER = "other"            # å…¶ä»–
```

#### 2. config_schema å±æ€§

```python
@property
@abstractmethod
def config_schema(self) -> List[ConfigField]:
    """è¿”å›æ’ä»¶é…ç½®è¡¨å•å®šä¹‰"""
    pass
```

#### 3. æ ¸å¿ƒæ–¹æ³•

```python
@abstractmethod
def validate_config(self, config: Dict[str, Any]) -> bool:
    """éªŒè¯é…ç½®å‚æ•°"""
    pass

@abstractmethod  
def test_connection(self, config: Dict[str, Any]) -> TestResult:
    """æµ‹è¯•æ•°æ®æºè¿æ¥"""
    pass

@abstractmethod
def crawl(self, config: Dict[str, Any], **kwargs) -> CrawlResult:
    """æ‰§è¡Œæ•°æ®çˆ¬å–"""
    pass
```

### å¯é€‰æ–¹æ³•

```python
def get_allowed_domains(self) -> List[str]:
    """è¿”å›å…è®¸è®¿é—®çš„åŸŸååˆ—è¡¨"""
    return []

def get_required_permissions(self) -> List[str]:
    """è¿”å›éœ€è¦çš„æƒé™åˆ—è¡¨"""
    return ["network"]

def cleanup(self):
    """æ’ä»¶å¸è½½æ—¶çš„æ¸…ç†å·¥ä½œ"""
    pass
```

---

## é…ç½®ç³»ç»Ÿè¯¦è§£

### ConfigField å®Œæ•´å‚è€ƒ

```python
ConfigField(
    name="field_name",          # å­—æ®µåç§°ï¼ˆè‹±æ–‡ï¼Œä»£ç ä¸­ä½¿ç”¨ï¼‰
    label="æ˜¾ç¤ºåç§°",            # ç•Œé¢æ˜¾ç¤ºæ ‡ç­¾
    type="field_type",          # å­—æ®µç±»å‹
    required=False,             # æ˜¯å¦å¿…å¡«
    default=None,               # é»˜è®¤å€¼
    placeholder="å ä½ç¬¦",        # è¾“å…¥æç¤º
    options=[],                 # é€‰æ‹©é¡¹ï¼ˆselectç±»å‹ï¼‰
    help_text="å¸®åŠ©è¯´æ˜",        # è¯¦ç»†è¯´æ˜
    validation={}               # éªŒè¯è§„åˆ™
)
```

### æ”¯æŒçš„å­—æ®µç±»å‹

#### 1. æ–‡æœ¬è¾“å…¥ç±»å‹

```python
# æ™®é€šæ–‡æœ¬
ConfigField(
    name="username",
    label="ç”¨æˆ·å",
    type="text",
    required=True,
    placeholder="è¯·è¾“å…¥ç”¨æˆ·å"
)

# å¯†ç è¾“å…¥
ConfigField(
    name="password",
    label="å¯†ç ",
    type="password",
    required=True,
    help_text="APIè®¿é—®å¯†ç "
)

# URLè¾“å…¥
ConfigField(
    name="api_url",
    label="APIåœ°å€",
    type="url",
    required=True,
    validation={
        "pattern": r"^https?://.*",
        "message": "è¯·è¾“å…¥æœ‰æ•ˆçš„URL"
    }
)

# å¤šè¡Œæ–‡æœ¬
ConfigField(
    name="description",
    label="æè¿°",
    type="textarea",
    placeholder="è¾“å…¥è¯¦ç»†æè¿°..."
)
```

#### 2. æ•°å­—è¾“å…¥ç±»å‹

```python
ConfigField(
    name="timeout",
    label="è¶…æ—¶æ—¶é—´(ç§’)",
    type="number",
    default=30,
    validation={
        "min": 1,
        "max": 300,
        "step": 1
    },
    help_text="è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼Œ1-300ç§’"
)
```

#### 3. é€‰æ‹©ç±»å‹

```python
# ä¸‹æ‹‰é€‰æ‹©
ConfigField(
    name="region",
    label="åœ°åŒº",
    type="select",
    default="cn",
    options=[
        {"value": "cn", "label": "ä¸­å›½"},
        {"value": "us", "label": "ç¾å›½"},
        {"value": "eu", "label": "æ¬§æ´²"}
    ]
)

# å•é€‰æŒ‰é’®
ConfigField(
    name="format",
    label="æ•°æ®æ ¼å¼",
    type="radio",
    default="json",
    options=[
        {"value": "json", "label": "JSON"},
        {"value": "xml", "label": "XML"}
    ]
)

# å¤šé€‰æ¡†
ConfigField(
    name="categories",
    label="ç±»åˆ«",
    type="checkbox-group",
    default=[],
    options=[
        {"value": "resistor", "label": "ç”µé˜»"},
        {"value": "capacitor", "label": "ç”µå®¹"}
    ]
)
```

#### 4. å¸ƒå°”ç±»å‹

```python
ConfigField(
    name="include_images",
    label="åŒ…å«å›¾ç‰‡",
    type="checkbox",
    default=True,
    help_text="æ˜¯å¦è·å–é›¶ä»¶å›¾ç‰‡"
)
```

#### 5. JSONç±»å‹

```python
ConfigField(
    name="headers",
    label="HTTPè¯·æ±‚å¤´",
    type="json",
    default='{"User-Agent": "OpenPart-Crawler"}',
    help_text="è‡ªå®šä¹‰HTTPè¯·æ±‚å¤´ï¼ŒJSONæ ¼å¼"
)
```

### éªŒè¯è§„åˆ™

```python
validation = {
    # å­—ç¬¦ä¸²é•¿åº¦
    "min_length": 5,
    "max_length": 100,
    
    # æ•°å­—èŒƒå›´
    "min": 0,
    "max": 1000,
    "step": 0.1,
    
    # æ­£åˆ™è¡¨è¾¾å¼
    "pattern": r"^[a-zA-Z0-9]+$",
    "message": "åªèƒ½åŒ…å«å­—æ¯å’Œæ•°å­—"
}
```

---

## å®‰å…¨çº¦æŸå’Œè§„èŒƒ

### ğŸš« ç¦ç”¨åŠŸèƒ½

ä¸ºäº†ç³»ç»Ÿå®‰å…¨ï¼Œæ’ä»¶ä»£ç ä¸­ç¦æ­¢ä½¿ç”¨ä»¥ä¸‹åŠŸèƒ½ï¼š

#### 1. ç¦ç”¨çš„å¯¼å…¥æ¨¡å—

```python
# ç¦æ­¢å¯¼å…¥çš„æ¨¡å—
import os          # âŒ ç³»ç»Ÿæ“ä½œ
import sys         # âŒ ç³»ç»Ÿæ“ä½œ
import subprocess  # âŒ è¿›ç¨‹æ‰§è¡Œ
import socket      # âŒ ç½‘ç»œåº•å±‚æ“ä½œ
import threading   # âŒ å¤šçº¿ç¨‹
import multiprocessing  # âŒ å¤šè¿›ç¨‹

# å…è®¸çš„å¯¼å…¥ç¤ºä¾‹
import requests    # âœ… HTTPè¯·æ±‚
import json        # âœ… JSONå¤„ç†
import time        # âœ… æ—¶é—´æ“ä½œ
import re          # âœ… æ­£åˆ™è¡¨è¾¾å¼
from urllib.parse import urljoin  # âœ… URLå¤„ç†
```

#### 2. ç¦ç”¨çš„å‡½æ•°è°ƒç”¨

```python
# ç¦æ­¢çš„å‡½æ•°
eval()         # âŒ åŠ¨æ€ä»£ç æ‰§è¡Œ
exec()         # âŒ åŠ¨æ€ä»£ç æ‰§è¡Œ
compile()      # âŒ ä»£ç ç¼–è¯‘
__import__()   # âŒ åŠ¨æ€å¯¼å…¥
open()         # âŒ æ–‡ä»¶æ“ä½œ
```

#### 3. ç¦ç”¨çš„å±æ€§è®¿é—®

```python
# ç¦æ­¢è®¿é—®çš„å±æ€§
obj.__class__      # âŒ ç±»ä¿¡æ¯
obj.__globals__    # âŒ å…¨å±€å˜é‡
obj.__code__       # âŒ ä»£ç å¯¹è±¡
```

### âœ… æ¨èçš„å®ç°æ–¹å¼

```python
# æ¨èçš„HTTPè¯·æ±‚
import requests
response = requests.get(url, timeout=30)

# æ¨èçš„æ•°æ®è§£æ
import json
data = json.loads(response.text)

# æ¨èçš„URLå¤„ç†
from urllib.parse import urljoin, urlparse
full_url = urljoin(base_url, relative_url)

# æ¨èçš„é”™è¯¯å¤„ç†
try:
    # ç½‘ç»œè¯·æ±‚
    response = requests.get(url)
    response.raise_for_status()
except requests.RequestException as e:
    raise NetworkError(f"è¯·æ±‚å¤±è´¥: {str(e)}")
```

---

## æ•°æ®æ¨¡å‹å’Œç±»å‹

### PartData - é›¶ä»¶æ•°æ®æ¨¡å‹

```python
class PartData(BaseModel):
    """é›¶ä»¶æ•°æ®æ ‡å‡†æ ¼å¼"""
    
    # å¿…éœ€å­—æ®µ
    name: str                           # é›¶ä»¶åç§°
    
    # å¯é€‰å­—æ®µ
    category: Optional[str] = None      # é›¶ä»¶ç±»åˆ«
    description: Optional[str] = None   # é›¶ä»¶æè¿°
    properties: Optional[Dict[str, Any]] = None  # è‡ªå®šä¹‰å±æ€§
    image_url: Optional[str] = None     # å›¾ç‰‡URL
    source_url: Optional[str] = None    # åŸå§‹æ•°æ®URL
    external_id: Optional[str] = None   # å¤–éƒ¨ç³»ç»ŸID
    price: Optional[float] = None       # ä»·æ ¼ä¿¡æ¯
    availability: Optional[str] = None  # åº“å­˜çŠ¶æ€
```

**ç¤ºä¾‹ï¼š**

```python
part = PartData(
    name="1kÎ©ç²¾å¯†ç”µé˜»",
    category="ç”µé˜»å™¨",
    description="1%ç²¾åº¦é‡‘å±è†œç”µé˜»ï¼ŒåŠŸç‡0.25W",
    properties={
        "é˜»å€¼": "1kÎ©",
        "åŠŸç‡": "0.25W", 
        "ç²¾åº¦": "Â±1%",
        "æ¸©åº¦ç³»æ•°": "Â±100ppm/Â°C",
        "å°è£…": "0805"
    },
    price=0.05,
    availability="ç°è´§",
    image_url="https://example.com/images/resistor_1k.jpg",
    source_url="https://example.com/products/res-1k-001"
)
```

### CrawlResult - çˆ¬å–ç»“æœæ¨¡å‹

```python
class CrawlResult(BaseModel):
    """çˆ¬å–ç»“æœ"""
    success: bool                       # æ˜¯å¦æˆåŠŸ
    data: List[PartData]                # çˆ¬å–çš„æ•°æ®åˆ—è¡¨
    total_count: int                    # æ€»æ•°æ®é‡
    error_message: Optional[str] = None # é”™è¯¯ä¿¡æ¯
    warnings: List[str] = []            # è­¦å‘Šä¿¡æ¯
    execution_time: Optional[float] = None  # æ‰§è¡Œæ—¶é—´(ç§’)
    next_page_token: Optional[str] = None   # ä¸‹ä¸€é¡µæ ‡è¯†
```

### TestResult - æµ‹è¯•ç»“æœæ¨¡å‹

```python
class TestResult(BaseModel):
    """æµ‹è¯•ç»“æœ"""
    success: bool                       # æµ‹è¯•æ˜¯å¦æˆåŠŸ
    message: str                        # æµ‹è¯•ç»“æœä¿¡æ¯
    response_time: Optional[float] = None   # å“åº”æ—¶é—´(ç§’)
    sample_data: Optional[Dict[str, Any]] = None  # ç¤ºä¾‹æ•°æ®
```

---

## APIæ¥å£è§„èŒƒ

### æ’ä»¶ç®¡ç†æ¥å£

#### 1. è·å–æ’ä»¶åˆ—è¡¨

```http
GET /api/admin/crawler-plugins/
Authorization: Bearer {token}
```

**å“åº”ï¼š**
```json
[
  {
    "id": 1,
    "name": "test_electronics",
    "display_name": "æµ‹è¯•ç”µå­å…ƒä»¶çˆ¬è™«",
    "version": "1.0.0",
    "description": "ç”¨äºæµ‹è¯•çš„ç”µå­å…ƒä»¶æ•°æ®çˆ¬è™«",
    "author": "OpenPart Team",
    "data_source": "æµ‹è¯•ç”µå­å•†åŸ",
    "status": "active",
    "is_active": true,
    "config": {
      "api_base_url": "https://api.test-electronics.com",
      "category_filter": "all"
    },
    "config_schema": [...],
    "allowed_domains": ["test-electronics.com"],
    "required_permissions": ["network"],
    "run_count": 5,
    "success_count": 4,
    "error_count": 1,
    "created_at": "2024-01-01T00:00:00Z"
  }
]
```

#### 2. ä¸Šä¼ æ’ä»¶

```http
POST /api/admin/crawler-plugins/upload
Authorization: Bearer {token}
Content-Type: multipart/form-data

plugin_file: {file.py}
```

#### 3. æ›´æ–°æ’ä»¶é…ç½®

```http
PUT /api/admin/crawler-plugins/{plugin_id}/config
Authorization: Bearer {token}
Content-Type: application/json

{
  "config": {
    "api_base_url": "https://api.example.com",
    "timeout": 30
  }
}
```

#### 4. æµ‹è¯•æ’ä»¶è¿æ¥

```http
POST /api/admin/crawler-plugins/{plugin_id}/test
Authorization: Bearer {token}
Content-Type: application/json

{
  "config": {
    "api_base_url": "https://api.example.com"
  }
}
```

**å“åº”ï¼š**
```json
{
  "success": true,
  "message": "è¿æ¥æµ‹è¯•æˆåŠŸ",
  "response_time": 1.23,
  "sample_data": {
    "server": "api-server",
    "version": "v2.1"
  }
}
```

### ä»»åŠ¡ç®¡ç†æ¥å£

#### 1. åˆ›å»ºä»»åŠ¡

```http
POST /api/admin/crawler-plugins/{plugin_id}/tasks
Authorization: Bearer {token}
Content-Type: application/json

{
  "name": "æ¯æ—¥çˆ¬å–ä»»åŠ¡",
  "description": "æ¯å¤©å®šæ—¶çˆ¬å–æœ€æ–°æ•°æ®",
  "schedule_type": "cron",
  "schedule_config": {
    "cron_expression": "0 0 * * *"
  },
  "config": {
    "category_filter": "resistor"
  }
}
```

#### 2. æ‰§è¡Œä»»åŠ¡

```http
POST /api/admin/crawler-plugins/{plugin_id}/tasks/{task_id}/execute
Authorization: Bearer {token}
```

#### 3. è·å–ä»»åŠ¡æ—¥å¿—

```http
GET /api/admin/crawler-plugins/tasks/{task_id}/logs
Authorization: Bearer {token}
```

---

## å›¾ç‰‡ä¸‹è½½æ¥å£

OpenPart æä¾›äº†å®‰å…¨çš„å›¾ç‰‡ä¸‹è½½APIï¼Œæ’ä»¶å¯ä»¥è°ƒç”¨æ­¤æ¥å£å°†è¿œç¨‹å›¾ç‰‡ä¸‹è½½åˆ°æœ¬åœ°æœåŠ¡å™¨ã€‚

### å›¾ç‰‡ä¸‹è½½API

#### æ¥å£è¯´æ˜

```http
POST /api/admin/images/download
Authorization: Bearer {admin_token}
Content-Type: application/json

{
    "part_id": 123,
    "image_url": "https://example.com/image.jpg",
    "replace_existing": true
}
```

**è¯·æ±‚å‚æ•°ï¼š**

| å­—æ®µ | ç±»å‹ | å¿…éœ€ | è¯´æ˜ |
|------|------|------|------|
| part_id | int | âœ… | é›¶ä»¶ID |
| image_url | string | âœ… | å›¾ç‰‡URLåœ°å€ |
| replace_existing | boolean | âŒ | æ˜¯å¦æ›¿æ¢å·²æœ‰å›¾ç‰‡ï¼ˆé»˜è®¤trueï¼‰ |

**å“åº”æ ¼å¼ï¼š**
```json
{
    "success": true,
    "message": "å›¾ç‰‡ä¸‹è½½æˆåŠŸ",
    "image_url": "/static/images/parts/part_123_abc123.jpg",
    "file_size": 245760,
    "content_type": "image/jpeg",
    "replaced_existing": false,
    "old_image_url": null
}
```

### å®‰å…¨é™åˆ¶

- **URLå®‰å…¨æ£€æŸ¥**ï¼šé˜»æ­¢è®¿é—®å†…ç½‘åœ°å€ã€localhost
- **æ–‡ä»¶å¤§å°é™åˆ¶**ï¼šæœ€å¤§5MB
- **æ–‡ä»¶ç±»å‹éªŒè¯**ï¼šä»…æ”¯æŒJPGã€PNGã€GIFã€WebPæ ¼å¼
- **å›¾ç‰‡æ ¼å¼éªŒè¯**ï¼šæ£€æŸ¥æ–‡ä»¶å¤´éƒ¨ç¡®ä¿ä¸ºçœŸå®å›¾ç‰‡
- **è‡ªåŠ¨ä¼˜åŒ–**ï¼šä¸‹è½½çš„å›¾ç‰‡ä¼šè‡ªåŠ¨è°ƒæ•´å¤§å°å¹¶è½¬æ¢ä¸ºJPEGæ ¼å¼

### æ’ä»¶ä¸­çš„ä½¿ç”¨ç¤ºä¾‹

```python
import requests

class MyPlugin(BaseCrawlerPlugin):
    def __init__(self):
        super().__init__()
        self.api_base = "http://localhost:8000/api"
        self.admin_token = None  # åœ¨å®é™…ä½¿ç”¨ä¸­ä¼šè¢«ç³»ç»Ÿæ³¨å…¥
    
    def download_part_image(self, part_id: int, image_url: str) -> Optional[str]:
        """ä¸ºé›¶ä»¶ä¸‹è½½å›¾ç‰‡"""
        try:
            response = requests.post(
                f"{self.api_base}/admin/images/download",
                headers={"Authorization": f"Bearer {self.admin_token}"},
                json={
                    "part_id": part_id,
                    "image_url": image_url,
                    "replace_existing": True
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("image_url")
            else:
                print(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {response.text}")
                return None
                
        except Exception as e:
            print(f"å›¾ç‰‡ä¸‹è½½å¼‚å¸¸: {str(e)}")
            return None
    
    def crawl(self, config: Dict[str, Any], **kwargs) -> CrawlResult:
        parts = []
        
        # çˆ¬å–æ•°æ®...
        for raw_data in self.fetch_raw_data(config):
            part = self.parse_part_data(raw_data)
            
            # å¦‚æœæœ‰å›¾ç‰‡URLï¼Œä¸‹è½½åˆ°æœ¬åœ°
            if part.image_url and config.get("download_images", False):
                # åˆ›å»ºé›¶ä»¶è®°å½•ä»¥è·å–ID
                part_id = self.create_part_record(part)
                
                # ä¸‹è½½å›¾ç‰‡
                local_image_url = self.download_part_image(part_id, part.image_url)
                if local_image_url:
                    part.image_url = local_image_url
            
            parts.append(part)
        
        return CrawlResult(success=True, data=parts, total_count=len(parts))
```

### URLå®‰å…¨æµ‹è¯•æ¥å£

```http
GET /api/admin/images/download/test?url=https://example.com/image.jpg
Authorization: Bearer {admin_token}
```

ç”¨äºæµ‹è¯•URLæ˜¯å¦ç¬¦åˆå®‰å…¨è¦æ±‚ã€‚

---

## æ–‡ä»¶ä¸Šä¼ å’Œæ‰¹é‡å¤„ç†æ¥å£

å¯¹äºå¤„ç†PDFäº§å“ç›®å½•ã€Excelä»·æ ¼è¡¨ç­‰æ–‡æ¡£æ•°æ®ï¼ŒOpenPartæä¾›äº†å®‰å…¨çš„æ–‡ä»¶ä¸Šä¼ å’Œæ‰¹é‡æ•°æ®å¤„ç†æ¥å£ã€‚

### å·¥ä½œæµç¨‹

```
æ’ä»¶ä¸Šä¼ æ–‡æ¡£ â†’ æœåŠ¡å™¨å®‰å…¨å­˜å‚¨ â†’ æ’ä»¶ä¸‹è½½è§£æ â†’ æäº¤ç»“æ„åŒ–æ•°æ® â†’ æ‰¹é‡å…¥åº“
```

### 1. æ–‡ä»¶ä¸Šä¼ æ¥å£

#### ä¸Šä¼ æ–‡ä»¶

```http
POST /api/admin/files/upload
Authorization: Bearer {admin_token}
Content-Type: multipart/form-data

file: {document_file}
```

**æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š**
- æ–‡æ¡£ï¼šPDF, DOC, DOCX
- è¡¨æ ¼ï¼šXLS, XLSX, CSV
- å‹ç¼©åŒ…ï¼šZIP, RAR, 7Z
- æ–‡æœ¬ï¼šTXT, JSON, XML

**å“åº”æ ¼å¼ï¼š**
```json
{
    "success": true,
    "message": "æ–‡ä»¶ä¸Šä¼ æˆåŠŸ",
    "file_id": "abc123def456",
    "filename": "parts_catalog.pdf",
    "file_size": 2048576,
    "content_type": "application/pdf",
    "download_url": "/api/admin/files/abc123def456/download",
    "expires_at": "2024-01-02T00:00:00Z"
}
```

#### ä¸‹è½½æ–‡ä»¶

```http
GET /api/admin/files/{file_id}/download
Authorization: Bearer {admin_token}
```

æ’ä»¶ä½¿ç”¨æ­¤æ¥å£ä¸‹è½½æ–‡ä»¶è¿›è¡Œè§£æã€‚

#### åˆ é™¤æ–‡ä»¶

```http
DELETE /api/admin/files/{file_id}
Authorization: Bearer {admin_token}
```

æ’ä»¶å¤„ç†å®Œæˆåè°ƒç”¨æ­¤æ¥å£æ¸…ç†ä¸´æ—¶æ–‡ä»¶ã€‚

### 2. æ‰¹é‡æ•°æ®åˆ›å»ºæ¥å£

#### æ‰¹é‡åˆ›å»ºé›¶ä»¶

```http
POST /api/admin/files/parts/batch-create
Authorization: Bearer {admin_token}
Content-Type: application/json

{
    "parts": [
        {
            "name": "é›¶ä»¶åç§°1",
            "category": "ç”µé˜»",
            "description": "é›¶ä»¶æè¿°",
            "properties": {"é˜»å€¼": "1kÎ©", "åŠŸç‡": "0.25W"},
            "price": 0.05,
            "image_url": null
        },
        {
            "name": "é›¶ä»¶åç§°2",
            "category": "ç”µå®¹",
            "description": "å¦ä¸€ä¸ªé›¶ä»¶",
            "properties": {"å®¹é‡": "100Î¼F", "ç”µå‹": "16V"}
        }
    ]
}
```

**å“åº”æ ¼å¼ï¼š**
```json
{
    "success": true,
    "message": "æ‰¹é‡å¤„ç†å®Œæˆ",
    "total_processed": 2,
    "successful_creates": 2,
    "skipped_duplicates": 0,
    "errors": []
}
```

### 3. å®‰å…¨é™åˆ¶

- **æ–‡ä»¶å¤§å°é™åˆ¶**ï¼šå•æ–‡ä»¶æœ€å¤§50MB
- **æ‰¹é‡æ•°æ®é™åˆ¶**ï¼šå•æ¬¡æœ€å¤šåˆ›å»º1000ä¸ªé›¶ä»¶
- **æ–‡ä»¶ç±»å‹éªŒè¯**ï¼šæ£€æŸ¥MIMEç±»å‹å’Œæ–‡ä»¶å¤´éƒ¨
- **ä¸´æ—¶å­˜å‚¨**ï¼šæ–‡ä»¶24å°æ—¶åè‡ªåŠ¨æ¸…ç†
- **æ¶æ„æ–‡ä»¶æ£€æµ‹**ï¼šé˜²æ­¢è·¯å¾„éå†å’Œæ–‡ä»¶ç‚¸å¼¹æ”»å‡»

### 4. æ’ä»¶ä½¿ç”¨ç¤ºä¾‹

```python
import requests
import io
import csv
import json

class DocumentProcessorPlugin(BaseCrawlerPlugin):
    """æ–‡æ¡£å¤„ç†æ’ä»¶ç¤ºä¾‹"""
    
    def __init__(self):
        super().__init__()
        self.api_base = "http://localhost:8000/api"
        self.admin_token = None
    
    @property
    def plugin_info(self) -> PluginInfo:
        return PluginInfo(
            name="æ–‡æ¡£å¤„ç†æ’ä»¶",
            version="1.0.0",
            description="å¤„ç†PDFäº§å“ç›®å½•å’ŒExcelä»·æ ¼è¡¨",
            author="Plugin Developer",
            data_source="Document Files",
            data_source_type=DataSourceType.DOCUMENT
        )
    
    @property
    def config_schema(self) -> List[ConfigField]:
        return [
            ConfigField(
                name="file_type",
                label="æ–‡ä»¶ç±»å‹",
                type="select",
                default="pdf",
                options=[
                    {"value": "pdf", "label": "PDFæ–‡æ¡£"},
                    {"value": "excel", "label": "Excelè¡¨æ ¼"},
                    {"value": "csv", "label": "CSVæ–‡ä»¶"}
                ]
            ),
            ConfigField(
                name="auto_categorize",
                label="è‡ªåŠ¨åˆ†ç±»",
                type="checkbox",
                default=True,
                help_text="æ ¹æ®äº§å“åç§°è‡ªåŠ¨åˆ†é…ç±»åˆ«"
            )
        ]
    
    def process_uploaded_file(self, file_path: str, config: Dict[str, Any]) -> List[PartData]:
        """å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶"""
        
        # 1. ä¸Šä¼ æ–‡ä»¶
        with open(file_path, 'rb') as f:
            files = {'file': f}
            response = requests.post(
                f"{self.api_base}/admin/files/upload",
                headers={"Authorization": f"Bearer {self.admin_token}"},
                files=files
            )
        
        if response.status_code != 200:
            raise Exception(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {response.text}")
        
        upload_result = response.json()
        file_id = upload_result['file_id']
        
        try:
            # 2. ä¸‹è½½å¹¶è§£ææ–‡ä»¶
            download_response = requests.get(
                f"{self.api_base}/admin/files/{file_id}/download",
                headers={"Authorization": f"Bearer {self.admin_token}"}
            )
            
            if download_response.status_code != 200:
                raise Exception(f"æ–‡ä»¶ä¸‹è½½å¤±è´¥: {download_response.text}")
            
            # 3. æ ¹æ®æ–‡ä»¶ç±»å‹è§£æå†…å®¹
            file_type = config.get("file_type", "pdf")
            if file_type == "csv":
                parts_data = self.parse_csv(download_response.content)
            elif file_type == "excel":
                parts_data = self.parse_excel(download_response.content)
            elif file_type == "pdf":
                parts_data = self.parse_pdf(download_response.content)
            else:
                raise Exception(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
            
            # 4. æ‰¹é‡åˆ›å»ºé›¶ä»¶
            if parts_data:
                create_response = requests.post(
                    f"{self.api_base}/admin/files/parts/batch-create",
                    headers={
                        "Authorization": f"Bearer {self.admin_token}",
                        "Content-Type": "application/json"
                    },
                    json={"parts": [part.dict() for part in parts_data]}
                )
                
                if create_response.status_code == 200:
                    result = create_response.json()
                    print(f"æ‰¹é‡åˆ›å»ºå®Œæˆ: æˆåŠŸ{result['successful_creates']}ä¸ª")
                    return parts_data
                else:
                    raise Exception(f"æ‰¹é‡åˆ›å»ºå¤±è´¥: {create_response.text}")
            
        finally:
            # 5. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            requests.delete(
                f"{self.api_base}/admin/files/{file_id}",
                headers={"Authorization": f"Bearer {self.admin_token}"}
            )
        
        return []
    
    def parse_csv(self, content: bytes) -> List[PartData]:
        """è§£æCSVæ–‡ä»¶"""
        parts = []
        csv_text = content.decode('utf-8')
        reader = csv.DictReader(io.StringIO(csv_text))
        
        for row in reader:
            if row.get('name'):  # ç¡®ä¿æœ‰åç§°
                properties = {}
                # æå–é™¤åŸºç¡€å­—æ®µå¤–çš„æ‰€æœ‰å­—æ®µä½œä¸ºå±æ€§
                for key, value in row.items():
                    if key not in ['name', 'category', 'description', 'price'] and value:
                        properties[key] = value
                
                part = PartData(
                    name=row['name'],
                    category=row.get('category'),
                    description=row.get('description'),
                    properties=properties if properties else None,
                    price=float(row['price']) if row.get('price') else None
                )
                parts.append(part)
        
        return parts
    
    def parse_excel(self, content: bytes) -> List[PartData]:
        """è§£æExcelæ–‡ä»¶ - éœ€è¦pandasæˆ–openpyxlåº“"""
        # æ³¨æ„ï¼šç”±äºå®‰å…¨é™åˆ¶ï¼Œå®é™…æ’ä»¶ä¸­éœ€è¦ä½¿ç”¨å…è®¸çš„åº“
        # è¿™é‡Œæä¾›ä¼ªä»£ç ç¤ºä¾‹
        parts = []
        
        # ä¼ªä»£ç  - å®é™…å®ç°éœ€è¦æ ¹æ®å¯ç”¨åº“è°ƒæ•´
        # workbook = load_excel(content)
        # for row in workbook.active.iter_rows(values_only=True):
        #     if row[0]:  # æœ‰åç§°
        #         parts.append(PartData(name=str(row[0]), ...))
        
        return parts
    
    def parse_pdf(self, content: bytes) -> List[PartData]:
        """è§£æPDFæ–‡ä»¶ - éœ€è¦PDFå¤„ç†åº“"""
        parts = []
        
        # ä¼ªä»£ç  - PDFè§£æé€šå¸¸æ¯”è¾ƒå¤æ‚
        # å¯èƒ½éœ€è¦ä½¿ç”¨OCRæŠ€æœ¯æˆ–ç‰¹å®šçš„PDFè§£æåº“
        # text = extract_text_from_pdf(content)
        # parts = extract_parts_from_text(text)
        
        return parts
    
    def crawl(self, config: Dict[str, Any], **kwargs) -> CrawlResult:
        """æ­¤æ’ä»¶é€šè¿‡æ–‡ä»¶å¤„ç†è€Œä¸æ˜¯ç½‘ç»œçˆ¬å–è·å–æ•°æ®"""
        # æ–‡æ¡£å¤„ç†æ’ä»¶é€šå¸¸é€šè¿‡ä¸Šä¼ æ–‡ä»¶æ¥å£å·¥ä½œ
        # è€Œä¸æ˜¯ä¼ ç»Ÿçš„ç½‘ç»œçˆ¬å–
        return CrawlResult(
            success=True,
            data=[],
            total_count=0,
            warnings=["æ­¤æ’ä»¶éœ€è¦é€šè¿‡æ–‡ä»¶ä¸Šä¼ æ¥å£ä½¿ç”¨"]
        )

# å¿…éœ€ï¼šåˆ›å»ºæ’ä»¶å®ä¾‹
plugin = DocumentProcessorPlugin()
```

### 5. ä½¿ç”¨åœºæ™¯ç¤ºä¾‹

#### åœºæ™¯1ï¼šå¤„ç†ä¾›åº”å•†ä»·æ ¼è¡¨
```python
# ä¾›åº”å•†æ¯æœˆå‘é€Excelä»·æ ¼è¡¨
def process_supplier_pricelist(self, excel_file_path: str):
    parts = self.process_uploaded_file(excel_file_path, {
        "file_type": "excel",
        "auto_categorize": True
    })
    return parts
```

#### åœºæ™¯2ï¼šè§£æPDFäº§å“ç›®å½•
```python
# å¤„ç†PDFæ ¼å¼çš„äº§å“ç›®å½•
def process_product_catalog(self, pdf_file_path: str):
    parts = self.process_uploaded_file(pdf_file_path, {
        "file_type": "pdf",
        "auto_categorize": False
    })
    return parts
```

#### åœºæ™¯3ï¼šæ‰¹é‡å¯¼å…¥CSVæ•°æ®
```python
# å¤„ç†æ ‡å‡†åŒ–çš„CSVæ•°æ®
def import_csv_data(self, csv_file_path: str):
    parts = self.process_uploaded_file(csv_file_path, {
        "file_type": "csv"
    })
    return parts
```

### 6. è·å–æ–‡ä»¶ç»Ÿè®¡æ¥å£

```http
GET /api/admin/files/stats
Authorization: Bearer {admin_token}
```

**å“åº”æ ¼å¼ï¼š**
```json
{
    "total_files": 3,
    "total_size": 15728640,
    "total_size_mb": 15.0,
    "files": [
        {
            "filename": "abc123_1640995200.pdf",
            "size": 2048576,
            "created_at": "2024-01-01T12:00:00",
            "modified_at": "2024-01-01T12:00:00"
        }
    ]
}
```

---

## å®Œæ•´ç¤ºä¾‹

### é«˜çº§ç”µå•†æ’ä»¶ç¤ºä¾‹ï¼ˆåŒ…å«å›¾ç‰‡ä¸‹è½½ï¼‰

```python
# advanced_ecommerce_plugin.py
"""
é«˜çº§ç”µå•†çˆ¬è™«æ’ä»¶ç¤ºä¾‹

å±•ç¤ºå®Œæ•´çš„æ’ä»¶å¼€å‘æœ€ä½³å®è·µï¼ŒåŒ…å«å›¾ç‰‡ä¸‹è½½åŠŸèƒ½
"""

import time
import requests
import json
import re
from typing import Dict, Any, List, Optional
from urllib.parse import urljoin, urlparse
from app.plugins.crawler_base import (
    BaseCrawlerPlugin, PluginInfo, ConfigField,
    CrawlResult, TestResult, PartData, DataSourceType
)

class AdvancedEcommercePlugin(BaseCrawlerPlugin):
    """é«˜çº§ç”µå•†çˆ¬è™«æ’ä»¶"""
    
    def __init__(self):
        super().__init__()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'OpenPart-Crawler/1.0'
        })
        self.api_base = "http://localhost:8000/api"
        self.admin_token = None  # ç”±ç³»ç»Ÿæ³¨å…¥
    
    @property
    def plugin_info(self) -> PluginInfo:
        return PluginInfo(
            name="é«˜çº§ç”µå•†çˆ¬è™«",
            version="2.1.0",
            description="åŠŸèƒ½å®Œæ•´çš„ç”µå•†ç½‘ç«™é›¶ä»¶æ•°æ®çˆ¬è™«ï¼Œæ”¯æŒå›¾ç‰‡ä¸‹è½½å’Œé«˜çº§é…ç½®",
            author="Plugin Developer",
            data_source="Generic E-commerce",
            data_source_type=DataSourceType.ECOMMERCE,
            homepage="https://ecommerce-site.com",
            rate_limit=2,
            batch_size=50
        )
    
    @property
    def config_schema(self) -> List[ConfigField]:
        return [
            # åŸºç¡€é…ç½®
            ConfigField(
                name="api_base_url",
                label="APIåŸºç¡€åœ°å€",
                type="url",
                required=True,
                placeholder="https://api.example.com",
                help_text="ç”µå•†ç½‘ç«™çš„APIåŸºç¡€åœ°å€",
                validation={"pattern": r"^https://.*"}
            ),
            
            ConfigField(
                name="api_key",
                label="APIå¯†é’¥",
                type="password",
                required=False,
                help_text="å¦‚æœéœ€è¦è®¤è¯è¯·å¡«å†™APIå¯†é’¥"
            ),
            
            # å›¾ç‰‡ä¸‹è½½é…ç½®
            ConfigField(
                name="download_images",
                label="ä¸‹è½½äº§å“å›¾ç‰‡",
                type="checkbox",
                default=True,
                help_text="å¯ç”¨åä¼šè‡ªåŠ¨ä¸‹è½½äº§å“å›¾ç‰‡åˆ°æœ¬åœ°æœåŠ¡å™¨"
            ),
            
            ConfigField(
                name="image_quality",
                label="å›¾ç‰‡è´¨é‡",
                type="select",
                default="medium",
                options=[
                    {"value": "high", "label": "é«˜è´¨é‡ï¼ˆåŸå§‹å°ºå¯¸ï¼‰"},
                    {"value": "medium", "label": "ä¸­ç­‰è´¨é‡ï¼ˆ800x600ï¼‰"},
                    {"value": "low", "label": "ä½è´¨é‡ï¼ˆ400x300ï¼‰"}
                ],
                help_text="é€‰æ‹©ä¸‹è½½å›¾ç‰‡çš„è´¨é‡ç­‰çº§"
            ),
            
            # æœç´¢é…ç½®
            ConfigField(
                name="search_categories",
                label="æœç´¢ç±»åˆ«",
                type="checkbox-group",
                default=["electronic"],
                options=[
                    {"value": "electronic", "label": "ç”µå­å…ƒä»¶"},
                    {"value": "mechanical", "label": "æœºæ¢°é›¶ä»¶"},
                    {"value": "sensor", "label": "ä¼ æ„Ÿå™¨"},
                    {"value": "connector", "label": "è¿æ¥å™¨"}
                ],
                help_text="é€‰æ‹©è¦çˆ¬å–çš„é›¶ä»¶ç±»åˆ«"
            ),
            
            # æ€§èƒ½é…ç½®
            ConfigField(
                name="request_delay",
                label="è¯·æ±‚å»¶è¿Ÿ(ç§’)",
                type="number",
                default=1.0,
                validation={"min": 0.1, "max": 10.0, "step": 0.1},
                help_text="è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿæ—¶é—´"
            )
        ]
    
    def validate_config(self, config: Dict[str, Any]) -> bool:
        """éªŒè¯é…ç½®å‚æ•°"""
        if not config.get("api_base_url"):
            raise ValueError("APIåŸºç¡€åœ°å€ä¸èƒ½ä¸ºç©º")
        
        # éªŒè¯URLæ ¼å¼
        try:
            result = urlparse(config["api_base_url"])
            if not all([result.scheme, result.netloc]):
                raise ValueError("APIåŸºç¡€åœ°å€æ ¼å¼ä¸æ­£ç¡®")
        except Exception:
            raise ValueError("APIåŸºç¡€åœ°å€æ ¼å¼ä¸æ­£ç¡®")
        
        return True
    
    def test_connection(self, config: Dict[str, Any]) -> TestResult:
        """æµ‹è¯•æ•°æ®æºè¿æ¥"""
        start_time = time.time()
        
        try:
            api_url = config["api_base_url"]
            headers = self._build_headers(config)
            
            # å°è¯•è¿æ¥å¥åº·æ£€æŸ¥ç«¯ç‚¹
            test_endpoint = urljoin(api_url, "/health")
            response = self.session.get(
                test_endpoint,
                headers=headers,
                timeout=10
            )
            
            response_time = time.time() - start_time
            
            if response.status_code == 200:
                try:
                    server_info = response.json()
                except:
                    server_info = {"status": "ok"}
                
                return TestResult(
                    success=True,
                    message=f"è¿æ¥æˆåŠŸï¼æœåŠ¡å™¨å“åº”æ­£å¸¸ (HTTP {response.status_code})",
                    response_time=round(response_time, 3),
                    sample_data=server_info
                )
            else:
                return TestResult(
                    success=False,
                    message=f"è¿æ¥å¤±è´¥ï¼šHTTP {response.status_code}"
                )
            
        except requests.Timeout:
            return TestResult(
                success=False,
                message="è¿æ¥è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–APIåœ°å€"
            )
        except requests.ConnectionError:
            return TestResult(
                success=False,
                message="æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ï¼Œè¯·æ£€æŸ¥APIåœ°å€"
            )
        except Exception as e:
            return TestResult(
                success=False,
                message=f"è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}"
            )
    
    def crawl(self, config: Dict[str, Any], **kwargs) -> CrawlResult:
        """æ‰§è¡Œæ•°æ®çˆ¬å–"""
        start_time = time.time()
        crawled_parts = []
        warnings = []
        
        try:
            api_url = config["api_base_url"]
            headers = self._build_headers(config)
            search_categories = config.get("search_categories", ["electronic"])
            download_images = config.get("download_images", False)
            
            # åˆ†é¡µå‚æ•°
            page_token = kwargs.get("page_token", "1")
            limit = kwargs.get("limit", config.get("batch_size", 50))
            
            # çˆ¬å–æ¯ä¸ªç±»åˆ«
            for category in search_categories:
                try:
                    category_parts = self._crawl_category(
                        api_url, headers, category, page_token, 
                        limit // len(search_categories)
                    )
                    
                    # å¤„ç†å›¾ç‰‡ä¸‹è½½
                    if download_images:
                        category_parts = self._process_images(category_parts, warnings)
                    
                    crawled_parts.extend(category_parts)
                    
                    # è¯·æ±‚å»¶è¿Ÿ
                    delay = config.get("request_delay", 1.0)
                    time.sleep(delay)
                    
                except Exception as e:
                    warnings.append(f"çˆ¬å–ç±»åˆ« {category} æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            execution_time = time.time() - start_time
            
            # è®¡ç®—ä¸‹ä¸€é¡µæ ‡è¯†
            next_page = None
            if len(crawled_parts) >= limit:
                current_page = int(page_token) if page_token.isdigit() else 1
                next_page = str(current_page + 1)
            
            return CrawlResult(
                success=True,
                data=crawled_parts,
                total_count=len(crawled_parts),
                execution_time=round(execution_time, 3),
                warnings=warnings,
                next_page_token=next_page
            )
            
        except Exception as e:
            return CrawlResult(
                success=False,
                data=crawled_parts,
                total_count=len(crawled_parts),
                error_message=f"çˆ¬å–è¿‡ç¨‹å‡ºé”™: {str(e)}",
                execution_time=time.time() - start_time,
                warnings=warnings
            )
    
    def _build_headers(self, config: Dict[str, Any]) -> Dict[str, str]:
        """æ„å»ºHTTPè¯·æ±‚å¤´"""
        headers = {
            'User-Agent': 'OpenPart-Crawler/2.1',
            'Accept': 'application/json',
            'Content-Type': 'application/json'
        }
        
        if config.get("api_key"):
            headers['Authorization'] = f'Bearer {config["api_key"]}'
        
        return headers
    
    def _crawl_category(self, api_url: str, headers: Dict, category: str,
                       page_token: str, limit: int) -> List[PartData]:
        """çˆ¬å–æŒ‡å®šç±»åˆ«çš„é›¶ä»¶"""
        parts = []
        
        # æ„å»ºæœç´¢URL
        search_url = urljoin(api_url, "/products/search")
        params = {
            'category': category,
            'page': page_token,
            'limit': limit
        }
        
        # å‘é€è¯·æ±‚
        response = self.session.get(
            search_url,
            headers=headers,
            params=params,
            timeout=30
        )
        response.raise_for_status()
        
        # è§£æå“åº”
        data = response.json()
        products = data.get('products', [])
        
        # å¤„ç†æ¯ä¸ªäº§å“
        for product in products:
            try:
                part = self._parse_product(product, category)
                if part:
                    parts.append(part)
            except Exception as e:
                continue
        
        return parts
    
    def _parse_product(self, product: Dict, category: str) -> Optional[PartData]:
        """è§£æäº§å“æ•°æ®ä¸ºé›¶ä»¶æ ¼å¼"""  
        try:
            name = product.get("name", "").strip()
            if not name:
                return None
            
            description = product.get("description", "").strip()
            
            # ä»·æ ¼å¤„ç†
            price = None
            price_str = product.get("price")
            if price_str:
                try:
                    price = float(re.sub(r'[^\d.]', '', str(price_str)))
                except ValueError:
                    pass
            
            # å›¾ç‰‡URL
            image_url = product.get("image_url")
            if image_url and not image_url.startswith('http'):
                image_url = urljoin(product.get("base_url", ""), image_url)
            
            # è‡ªå®šä¹‰å±æ€§
            properties = {}
            specs = product.get("specifications", {})
            if isinstance(specs, dict):
                for key, value in specs.items():
                    if value:
                        properties[str(key)] = str(value)
            
            return PartData(
                name=name,
                category=self._normalize_category(category),
                description=description,
                properties=properties if properties else None,
                price=price,
                image_url=image_url,
                source_url=product.get("url"),
                external_id=str(product.get("id", ""))
            )
            
        except Exception as e:
            return None
    
    def _normalize_category(self, category: str) -> str:
        """æ ‡å‡†åŒ–ç±»åˆ«åç§°"""
        category_map = {
            "electronic": "ç”µå­å…ƒä»¶",
            "mechanical": "æœºæ¢°é›¶ä»¶", 
            "sensor": "ä¼ æ„Ÿå™¨",
            "connector": "è¿æ¥å™¨"
        }
        return category_map.get(category, category)
    
    def _process_images(self, parts: List[PartData], warnings: List[str]) -> List[PartData]:
        """å¤„ç†å›¾ç‰‡ä¸‹è½½"""
        processed_parts = []
        
        for part in parts:
            if part.image_url:
                try:
                    # é¦–å…ˆéœ€è¦åˆ›å»ºé›¶ä»¶è®°å½•ä»¥è·å–part_id
                    # è¿™é‡Œå‡è®¾æœ‰ä¸€ä¸ªä¸´æ—¶åˆ›å»ºæ–¹æ³•
                    part_id = self._create_temp_part_record(part)
                    
                    # è°ƒç”¨å›¾ç‰‡ä¸‹è½½API
                    local_image_url = self._download_part_image(part_id, part.image_url)
                    if local_image_url:
                        part.image_url = local_image_url
                    else:
                        warnings.append(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {part.name}")
                        
                except Exception as e:
                    warnings.append(f"å¤„ç†å›¾ç‰‡æ—¶å‡ºé”™ {part.name}: {str(e)}")
            
            processed_parts.append(part)
        
        return processed_parts
    
    def _download_part_image(self, part_id: int, image_url: str) -> Optional[str]:
        """è°ƒç”¨å›¾ç‰‡ä¸‹è½½API"""
        try:
            response = requests.post(
                f"{self.api_base}/admin/images/download",
                headers={"Authorization": f"Bearer {self.admin_token}"},
                json={
                    "part_id": part_id,
                    "image_url": image_url,
                    "replace_existing": True
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("image_url")
            else:
                return None
                
        except Exception as e:
            return None
    
    def _create_temp_part_record(self, part: PartData) -> int:
        """åˆ›å»ºä¸´æ—¶é›¶ä»¶è®°å½•ï¼ˆç¤ºä¾‹æ–¹æ³•ï¼‰"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…APIå®ç°
        # è¿”å›åˆ›å»ºçš„é›¶ä»¶ID
        return 1  # ç¤ºä¾‹è¿”å›
    
    def get_allowed_domains(self) -> List[str]:
        """è¿”å›å…è®¸è®¿é—®çš„åŸŸå"""
        return [
            "api.ecommerce-site.com",
            "cdn.ecommerce-site.com",
            "img.ecommerce-site.com"
        ]
    
    def get_required_permissions(self) -> List[str]:
        """è¿”å›éœ€è¦çš„æƒé™"""
        return ["network"]
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'session'):
            self.session.close()

# å¿…éœ€ï¼šåˆ›å»ºæ’ä»¶å®ä¾‹
plugin = AdvancedEcommercePlugin()
```

---

## æœ€ä½³å®è·µ

### 1. ä»£ç ç»„ç»‡

```python
class MyPlugin(BaseCrawlerPlugin):
    """æ’ä»¶ä¸»ç±»"""
    
    def __init__(self):
        super().__init__()
        # åˆå§‹åŒ–ä¼šè¯ã€ç¼“å­˜ç­‰èµ„æº
        self.session = requests.Session()
        self.cache = {}
        self.api_base = "http://localhost:8000/api"
        self.admin_token = None
    
    # å°†å¤æ‚é€»è¾‘æ‹†åˆ†ä¸ºç§æœ‰æ–¹æ³•
    def _build_headers(self, config):
        """æ„å»ºè¯·æ±‚å¤´"""
        pass
    
    def _parse_response(self, response):
        """è§£æå“åº”æ•°æ®"""
        pass
    
    def _validate_data(self, data):
        """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
        pass
```

### 2. é”™è¯¯å¤„ç†

```python
def test_connection(self, config: Dict[str, Any]) -> TestResult:
    try:
        # æ ¸å¿ƒé€»è¾‘
        response = requests.get(url, timeout=10)
        return TestResult(success=True, message="è¿æ¥æˆåŠŸ")
        
    except requests.Timeout:
        return TestResult(success=False, message="è¿æ¥è¶…æ—¶")
    except requests.ConnectionError:
        return TestResult(success=False, message="æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨")
    except Exception as e:
        return TestResult(success=False, message=f"æœªçŸ¥é”™è¯¯: {str(e)}")
```

### 3. å›¾ç‰‡å¤„ç†æœ€ä½³å®è·µ

```python
def _safe_download_image(self, part_id: int, image_url: str) -> Optional[str]:
    """å®‰å…¨çš„å›¾ç‰‡ä¸‹è½½"""
    try:
        # éªŒè¯URLæ ¼å¼
        if not image_url or not image_url.startswith(('http://', 'https://')):
            return None
        
        # è°ƒç”¨ç³»ç»Ÿå›¾ç‰‡ä¸‹è½½API
        response = requests.post(
            f"{self.api_base}/admin/images/download",
            headers={"Authorization": f"Bearer {self.admin_token}"},
            json={
                "part_id": part_id,
                "image_url": image_url,
                "replace_existing": True
            },
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            if result.get("success"):
                return result.get("image_url")
        
        return None
        
    except Exception as e:
        # è®°å½•é”™è¯¯ä½†ä¸ä¸­æ–­ä¸»æµç¨‹
        print(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {str(e)}")
        return None
```

### 4. æ–‡ä»¶å¤„ç†æœ€ä½³å®è·µ

```python
def _safe_process_file(self, file_path: str, file_type: str) -> List[PartData]:
    """å®‰å…¨çš„æ–‡ä»¶å¤„ç†"""
    file_id = None
    try:
        # 1. ä¸Šä¼ æ–‡ä»¶
        with open(file_path, 'rb') as f:
            files = {'file': f}
            response = requests.post(
                f"{self.api_base}/admin/files/upload",
                headers={"Authorization": f"Bearer {self.admin_token}"},
                files=files
            )
        
        if response.status_code != 200:
            raise Exception(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {response.text}")
        
        upload_result = response.json()
        file_id = upload_result['file_id']
        
        # 2. ä¸‹è½½å¹¶è§£æ
        download_response = requests.get(
            f"{self.api_base}/admin/files/{file_id}/download",
            headers={"Authorization": f"Bearer {self.admin_token}"}
        )
        
        if download_response.status_code == 200:
            return self._parse_file_content(download_response.content, file_type)
        else:
            raise Exception("æ–‡ä»¶ä¸‹è½½å¤±è´¥")
            
    except Exception as e:
        print(f"æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}")
        return []
        
    finally:
        # 3. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if file_id:
            try:
                requests.delete(
                    f"{self.api_base}/admin/files/{file_id}",
                    headers={"Authorization": f"Bearer {self.admin_token}"}
                )
            except:
                pass  # æ¸…ç†å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
```

### 5. æ‰¹é‡æ•°æ®å¤„ç†

```python
def _batch_create_parts(self, parts: List[PartData]) -> Dict[str, Any]:
    """æ‰¹é‡åˆ›å»ºé›¶ä»¶"""
    try:
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        parts_data = []
        for part in parts[:1000]:  # é™åˆ¶æ•°é‡
            part_dict = {
                "name": part.name,
                "category": part.category,
                "description": part.description,
                "properties": part.properties,
                "image_url": part.image_url,
                "price": part.price
            }
            # è¿‡æ»¤Noneå€¼
            part_dict = {k: v for k, v in part_dict.items() if v is not None}
            parts_data.append(part_dict)
        
        # å‘é€æ‰¹é‡åˆ›å»ºè¯·æ±‚
        response = requests.post(
            f"{self.api_base}/admin/files/parts/batch-create",
            headers={
                "Authorization": f"Bearer {self.admin_token}",
                "Content-Type": "application/json"
            },
            json={"parts": parts_data}
        )
        
        if response.status_code == 200:
            return response.json()
        else:
            raise Exception(f"æ‰¹é‡åˆ›å»ºå¤±è´¥: {response.text}")
            
    except Exception as e:
        return {
            "success": False,
            "error_message": str(e),
            "total_processed": 0,
            "successful_creates": 0
        }
```

---

## è°ƒè¯•å’Œæµ‹è¯•

### 1. æœ¬åœ°æµ‹è¯•

```python
# test_my_plugin.py
from my_plugin import plugin

# æµ‹è¯•é…ç½®éªŒè¯
config = {
    "api_url": "https://api.example.com",
    "timeout": 30,
    "download_images": True
}

try:
    plugin.validate_config(config)
    print("âœ… é…ç½®éªŒè¯é€šè¿‡")
except ValueError as e:
    print(f"âŒ é…ç½®é”™è¯¯: {e}")

# æµ‹è¯•è¿æ¥
result = plugin.test_connection(config)
print(f"è¿æ¥æµ‹è¯•: {'æˆåŠŸ' if result.success else 'å¤±è´¥'}")
print(f"æ¶ˆæ¯: {result.message}")

# æµ‹è¯•çˆ¬å–
crawl_result = plugin.crawl(config, limit=5)
print(f"çˆ¬å–ç»“æœ: {len(crawl_result.data)} æ¡æ•°æ®")
```

### 2. å›¾ç‰‡ä¸‹è½½æµ‹è¯•

```python
def test_image_download():
    """æµ‹è¯•å›¾ç‰‡ä¸‹è½½åŠŸèƒ½"""
    test_urls = [
        "https://example.com/image1.jpg",
        "https://example.com/image2.png",
        "invalid-url",  # æµ‹è¯•é”™è¯¯å¤„ç†
    ]
    
    for i, url in enumerate(test_urls):
        result = plugin._safe_download_image(part_id=i+1, image_url=url)
        print(f"å›¾ç‰‡ {i+1}: {'æˆåŠŸ' if result else 'å¤±è´¥'} - {url}")
```

### 3. æ–‡ä»¶å¤„ç†æµ‹è¯•

```python
def test_file_processing():
    """æµ‹è¯•æ–‡ä»¶å¤„ç†åŠŸèƒ½"""
    test_files = [
        ("test_data.csv", "csv"),
        ("product_catalog.pdf", "pdf"),
        ("price_list.xlsx", "excel")
    ]
    
    for file_path, file_type in test_files:
        if os.path.exists(file_path):
            parts = plugin._safe_process_file(file_path, file_type)
            print(f"æ–‡ä»¶ {file_path}: è§£æå‡º {len(parts)} ä¸ªé›¶ä»¶")
```

---

## å¸¸è§é—®é¢˜

### Q: å¦‚ä½•åœ¨æ’ä»¶ä¸­ä½¿ç”¨å›¾ç‰‡ä¸‹è½½åŠŸèƒ½ï¼Ÿ

**A:** ä½¿ç”¨ç³»ç»Ÿæä¾›çš„å›¾ç‰‡ä¸‹è½½APIï¼š

```python
def download_product_image(self, part_id: int, image_url: str) -> Optional[str]:
    """ä¸‹è½½äº§å“å›¾ç‰‡åˆ°æœ¬åœ°"""
    try:
        response = requests.post(
            f"{self.api_base}/admin/images/download",
            headers={"Authorization": f"Bearer {self.admin_token}"},
            json={
                "part_id": part_id,
                "image_url": image_url,
                "replace_existing": True
            }
        )
        
        if response.status_code == 200:
            result = response.json()
            return result.get("image_url")  # è¿”å›æœ¬åœ°å›¾ç‰‡URL
        else:
            return None
    except Exception:
        return None
```

### Q: å¦‚ä½•å¤„ç†PDFæˆ–Excelæ–‡ä»¶ï¼Ÿ

**A:** ä½¿ç”¨æ–‡ä»¶ä¸Šä¼ å’Œæ‰¹é‡å¤„ç†APIï¼š

```python
def process_document(self, file_path: str) -> List[PartData]:
    """å¤„ç†æ–‡æ¡£æ–‡ä»¶"""
    # 1. ä¸Šä¼ æ–‡ä»¶
    with open(file_path, 'rb') as f:
        upload_response = requests.post(
            f"{self.api_base}/admin/files/upload",
            headers={"Authorization": f"Bearer {self.admin_token}"},
            files={'file': f}
        )
    
    if upload_response.status_code != 200:
        return []
    
    file_id = upload_response.json()['file_id']
    
    try:
        # 2. ä¸‹è½½å¹¶è§£æ
        download_response = requests.get(
            f"{self.api_base}/admin/files/{file_id}/download",
            headers={"Authorization": f"Bearer {self.admin_token}"}
        )
        
        # 3. è§£æå†…å®¹ï¼ˆæ ¹æ®æ–‡ä»¶ç±»å‹ï¼‰
        parts_data = self.parse_file_content(download_response.content)
        
        # 4. æ‰¹é‡åˆ›å»ºé›¶ä»¶
        if parts_data:
            batch_response = requests.post(
                f"{self.api_base}/admin/files/parts/batch-create",
                headers={
                    "Authorization": f"Bearer {self.admin_token}",
                    "Content-Type": "application/json"
                },
                json={"parts": [part.dict() for part in parts_data]}
            )
            
            if batch_response.status_code == 200:
                result = batch_response.json()
                print(f"æˆåŠŸåˆ›å»º {result['successful_creates']} ä¸ªé›¶ä»¶")
        
        return parts_data
        
    finally:
        # 5. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        requests.delete(
            f"{self.api_base}/admin/files/{file_id}",
            headers={"Authorization": f"Bearer {self.admin_token}"}
        )
```

### Q: å¦‚ä½•å¤„ç†å¤§é‡æ•°æ®è€Œä¸è¶…å‡ºå†…å­˜é™åˆ¶ï¼Ÿ

**A:** é‡‡ç”¨åˆ†æ‰¹å¤„ç†ç­–ç•¥ï¼š

```python
def crawl_large_dataset(self, config: Dict[str, Any]) -> CrawlResult:
    """å¤„ç†å¤§é‡æ•°æ®"""
    all_parts = []
    batch_size = 100
    page = 1
    
    while True:
        # åˆ†é¡µè·å–æ•°æ®
        batch_parts = self.fetch_page_data(config, page, batch_size)
        
        if not batch_parts:
            break
            
        # ç«‹å³å¤„ç†å›¾ç‰‡ä¸‹è½½
        if config.get("download_images"):
            batch_parts = self.process_batch_images(batch_parts)
        
        # ç«‹å³æ‰¹é‡åˆ›å»ºé›¶ä»¶
        self.batch_create_parts(batch_parts)
        
        all_parts.extend(batch_parts)
        page += 1
        
        # å†…å­˜æ§åˆ¶ï¼šé™åˆ¶æ€»æ•°é‡
        if len(all_parts) >= 1000:
            break
        
        time.sleep(1)  # é¿å…è¿‡å¿«è¯·æ±‚
    
    return CrawlResult(success=True, data=all_parts, total_count=len(all_parts))
```

### Q: å¦‚ä½•å¤„ç†ç½‘ç«™åçˆ¬è™«æœºåˆ¶ï¼Ÿ

**A:** ä½¿ç”¨ä»¥ä¸‹ç­–ç•¥ï¼š

```python
class AntiCrawlerPlugin(BaseCrawlerPlugin):
    def __init__(self):
        super().__init__()
        self.session = requests.Session()
        # è®¾ç½®çœŸå®æµè§ˆå™¨User-Agent
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def crawl_with_protection(self, config: Dict[str, Any]) -> CrawlResult:
        """å¸¦åçˆ¬ä¿æŠ¤çš„çˆ¬å–"""
        parts = []
        
        for url in self.get_target_urls(config):
            try:
                # éšæœºå»¶è¿Ÿ
                delay = random.uniform(1, 3)
                time.sleep(delay)
                
                # ä½¿ç”¨ä¼šè¯ä¿æŒCookie
                response = self.session.get(url, timeout=30)
                
                # æ£€æµ‹åçˆ¬è™«å“åº”
                if self.is_blocked_response(response):
                    # ç­‰å¾…æ›´é•¿æ—¶é—´åé‡è¯•
                    time.sleep(10)
                    response = self.session.get(url, timeout=30)
                
                # è§£ææ•°æ®
                page_parts = self.parse_page(response)
                parts.extend(page_parts)
                
            except Exception as e:
                continue
        
        return CrawlResult(success=True, data=parts, total_count=len(parts))
    
    def is_blocked_response(self, response) -> bool:
        """æ£€æµ‹æ˜¯å¦è¢«åçˆ¬è™«æ‹¦æˆª"""
        # æ£€æŸ¥çŠ¶æ€ç 
        if response.status_code in [403, 429, 503]:
            return True
        
        # æ£€æŸ¥é¡µé¢å†…å®¹
        content = response.text.lower()
        block_indicators = ['captcha', 'blocked', 'forbidden', 'éªŒè¯ç ']
        return any(indicator in content for indicator in block_indicators)
```

### Q: å¦‚ä½•ä¼˜åŒ–æ’ä»¶æ€§èƒ½ï¼Ÿ

**A:** ä»¥ä¸‹æ˜¯æ€§èƒ½ä¼˜åŒ–å»ºè®®ï¼š

```python
class OptimizedPlugin(BaseCrawlerPlugin):
    def __init__(self):
        super().__init__()
        # ä½¿ç”¨è¿æ¥æ± 
        self.session = requests.Session()
        adapter = requests.adapters.HTTPAdapter(
            pool_connections=10,
            pool_maxsize=20,
            max_retries=3
        )
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)
        
        # ç¼“å­˜æœºåˆ¶
        self.cache = {}
    
    def crawl_optimized(self, config: Dict[str, Any]) -> CrawlResult:
        """ä¼˜åŒ–çš„çˆ¬å–æ–¹æ³•"""
        parts = []
        
        # 1. æ‰¹é‡è·å–URLåˆ—è¡¨
        urls = self.get_all_urls(config)
        
        # 2. åˆ†æ‰¹å¤„ç†
        batch_size = 20
        for i in range(0, len(urls), batch_size):
            batch_urls = urls[i:i + batch_size]
            batch_parts = self.process_url_batch(batch_urls)
            parts.extend(batch_parts)
            
            # æ‰¹é‡å¤„ç†å›¾ç‰‡
            if config.get("download_images"):
                self.batch_process_images(batch_parts)
            
            # è¿›åº¦æŠ¥å‘Š
            progress = (i + batch_size) / len(urls) * 100
            print(f"è¿›åº¦: {progress:.1f}%")
        
        return CrawlResult(success=True, data=parts, total_count=len(parts))
    
    def process_url_batch(self, urls: List[str]) -> List[PartData]:
        """æ‰¹é‡å¤„ç†URL"""
        parts = []
        
        for url in urls:
            # æ£€æŸ¥ç¼“å­˜
            if url in self.cache:
                parts.append(self.cache[url])
                continue
            
            try:
                response = self.session.get(url, timeout=10)
                part = self.parse_product_page(response)
                if part:
                    self.cache[url] = part  # ç¼“å­˜ç»“æœ
                    parts.append(part)
            except Exception:
                continue
        
        return parts
```

### Q: å¦‚ä½•å¤„ç†æ’ä»¶é…ç½®éªŒè¯ï¼Ÿ

**A:** å®ç°å…¨é¢çš„é…ç½®éªŒè¯ï¼š

```python
def validate_config(self, config: Dict[str, Any]) -> bool:
    """å…¨é¢çš„é…ç½®éªŒè¯"""
    
    # 1. å¿…éœ€å­—æ®µæ£€æŸ¥
    required_fields = ["api_url", "timeout"]
    for field in required_fields:
        if not config.get(field):
            raise ValueError(f"{field} ä¸èƒ½ä¸ºç©º")
    
    # 2. ç±»å‹æ£€æŸ¥
    if not isinstance(config.get("timeout"), (int, float)):
        raise ValueError("timeout å¿…é¡»æ˜¯æ•°å­—")
    
    # 3. èŒƒå›´æ£€æŸ¥
    timeout = config["timeout"]
    if timeout < 1 or timeout > 300:
        raise ValueError("timeout å¿…é¡»åœ¨ 1-300 ç§’ä¹‹é—´")
    
    # 4. URLæ ¼å¼éªŒè¯
    api_url = config["api_url"]
    try:
        result = urlparse(api_url)
        if not all([result.scheme, result.netloc]):
            raise ValueError("APIåœ°å€æ ¼å¼ä¸æ­£ç¡®")
    except Exception:
        raise ValueError("APIåœ°å€æ ¼å¼ä¸æ­£ç¡®")
    
    # 5. JSONé…ç½®éªŒè¯
    json_fields = ["custom_headers", "advanced_settings"]
    for field in json_fields:
        if field in config and config[field]:
            try:
                json.loads(config[field])
            except json.JSONDecodeError:
                raise ValueError(f"{field} å¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
    
    # 6. æšä¸¾å€¼éªŒè¯
    if "image_quality" in config:
        valid_qualities = ["low", "medium", "high"]
        if config["image_quality"] not in valid_qualities:
            raise ValueError(f"image_quality å¿…é¡»æ˜¯: {', '.join(valid_qualities)}")
    
    return True
```

### Q: å¦‚ä½•å¤„ç†æ’ä»¶çš„é”™è¯¯å’Œå¼‚å¸¸ï¼Ÿ

**A:** å»ºç«‹å®Œå–„çš„é”™è¯¯å¤„ç†æœºåˆ¶ï¼š

```python
class RobustPlugin(BaseCrawlerPlugin):
    def __init__(self):
        super().__init__()
        self.error_count = 0
        self.max_errors = 10
    
    def crawl(self, config: Dict[str, Any], **kwargs) -> CrawlResult:
        """å¥å£®çš„çˆ¬å–æ–¹æ³•"""
        parts = []
        warnings = []
        
        try:
            urls = self.get_target_urls(config)
            
            for i, url in enumerate(urls):
                try:
                    part = self.crawl_single_url(url, config)
                    if part:
                        parts.append(part)
                    
                    # é‡ç½®é”™è¯¯è®¡æ•°
                    self.error_count = 0
                    
                except requests.RequestException as e:
                    # ç½‘ç»œé”™è¯¯
                    self.error_count += 1
                    warning = f"ç½‘ç»œé”™è¯¯ {url}: {str(e)}"
                    warnings.append(warning)
                    
                    if self.error_count >= self.max_errors:
                        return CrawlResult(
                            success=False,
                            data=parts,
                            total_count=len(parts),
                            error_message=f"è¿ç»­ç½‘ç»œé”™è¯¯è¿‡å¤šï¼Œå·²åœæ­¢çˆ¬å–",
                            warnings=warnings
                        )
                    
                    # ç½‘ç»œé”™è¯¯æ—¶ç­‰å¾…æ›´é•¿æ—¶é—´
                    time.sleep(5)
                    
                except json.JSONDecodeError as e:
                    # æ•°æ®è§£æé”™è¯¯
                    warnings.append(f"æ•°æ®è§£æé”™è¯¯ {url}: {str(e)}")
                    continue
                    
                except Exception as e:
                    # å…¶ä»–æœªé¢„æœŸé”™è¯¯
                    warnings.append(f"æœªçŸ¥é”™è¯¯ {url}: {str(e)}")
                    continue
            
            return CrawlResult(
                success=True,
                data=parts,
                total_count=len(parts),
                warnings=warnings
            )
            
        except Exception as e:
            # å…¨å±€é”™è¯¯
            return CrawlResult(
                success=False,
                data=parts,
                total_count=len(parts),
                error_message=f"çˆ¬å–è¿‡ç¨‹å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}",
                warnings=warnings
            )
    
    def crawl_single_url(self, url: str, config: Dict[str, Any]) -> Optional[PartData]:
        """çˆ¬å–å•ä¸ªURLçš„æ•°æ®"""
        response = requests.get(url, timeout=config.get("timeout", 30))
        response.raise_for_status()  # æŠ›å‡ºHTTPé”™è¯¯
        
        data = response.json()  # å¯èƒ½æŠ›å‡ºJSONè§£æé”™è¯¯
        
        return self.parse_product_data(data)  # å¯èƒ½æŠ›å‡ºæ•°æ®å¤„ç†é”™è¯¯
```

### Q: æ’ä»¶å¦‚ä½•å¤„ç†è®¤è¯å’Œä¼šè¯ç®¡ç†ï¼Ÿ

**A:** å®ç°å®‰å…¨çš„è®¤è¯æœºåˆ¶ï¼š

```python
class AuthenticatedPlugin(BaseCrawlerPlugin):
    def __init__(self):
        super().__init__()
        self.session = requests.Session()
        self.access_token = None
        self.token_expires_at = None
    
    def authenticate(self, config: Dict[str, Any]) -> bool:
        """å¤„ç†è®¤è¯"""
        try:
            auth_url = urljoin(config["api_base_url"], "/auth/login")
            
            auth_data = {
                "username": config.get("username"),
                "password": config.get("password"),
                "api_key": config.get("api_key")
            }
            
            response = self.session.post(auth_url, json=auth_data)
            response.raise_for_status()
            
            auth_result = response.json()
            self.access_token = auth_result.get("access_token")
            
            # è®¡ç®—tokenè¿‡æœŸæ—¶é—´
            expires_in = auth_result.get("expires_in", 3600)
            self.token_expires_at = time.time() + expires_in
            
            # è®¾ç½®è®¤è¯å¤´
            self.session.headers["Authorization"] = f"Bearer {self.access_token}"
            
            return True
            
        except Exception as e:
            print(f"è®¤è¯å¤±è´¥: {str(e)}")
            return False
    
    def is_token_valid(self) -> bool:
        """æ£€æŸ¥tokenæ˜¯å¦æœ‰æ•ˆ"""
        if not self.access_token or not self.token_expires_at:
            return False
        
        # æå‰5åˆ†é’Ÿåˆ·æ–°token
        return time.time() < (self.token_expires_at - 300)
    
    def ensure_authenticated(self, config: Dict[str, Any]) -> bool:
        """ç¡®ä¿å·²è®¤è¯"""
        if not self.is_token_valid():
            return self.authenticate(config)
        return True
    
    def crawl(self, config: Dict[str, Any], **kwargs) -> CrawlResult:
        """å¸¦è®¤è¯çš„çˆ¬å–"""
        # ç¡®ä¿å·²è®¤è¯
        if not self.ensure_authenticated(config):
            return CrawlResult(
                success=False,
                data=[],
                total_count=0,
                error_message="è®¤è¯å¤±è´¥ï¼Œæ— æ³•ç»§ç»­çˆ¬å–"
            )
        
        # è¿›è¡Œæ­£å¸¸çˆ¬å–
        return super().crawl(config, **kwargs)
```

---

## æ€»ç»“

OpenPart æ’ä»¶ç³»ç»Ÿä¸ºå¼€å‘è€…æä¾›äº†ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§ã€å®‰å…¨å¯é çš„æ•°æ®é‡‡é›†å¹³å°ã€‚é€šè¿‡æœ¬æŒ‡å—ï¼Œä½ ç°åœ¨å¯ä»¥ï¼š

### ğŸš€ æ ¸å¿ƒèƒ½åŠ›
- **å¿«é€Ÿå¼€å‘**ï¼šä½¿ç”¨æ ‡å‡†åŒ–çš„æ’ä»¶æ¡†æ¶
- **å®‰å…¨è¿è¡Œ**ï¼šåœ¨æ²™ç®±ç¯å¢ƒä¸­å®‰å…¨æ‰§è¡Œ
- **çµæ´»é…ç½®**ï¼šé€šè¿‡å¯è§†åŒ–ç•Œé¢è¿›è¡Œé…ç½®
- **å®æ—¶ç›‘æ§**ï¼šå®Œæ•´çš„ä»»åŠ¡æ‰§è¡Œå’Œæ—¥å¿—ç³»ç»Ÿ

### ğŸ¯ æ–°å¢ç‰¹æ€§
- **å›¾ç‰‡ä¸‹è½½**ï¼šå®‰å…¨åœ°å°†è¿œç¨‹å›¾ç‰‡ä¸‹è½½åˆ°æœ¬åœ°æœåŠ¡å™¨
- **æ–‡ä»¶å¤„ç†**ï¼šæ”¯æŒPDFã€Excelç­‰æ–‡æ¡£çš„è§£æå’Œæ‰¹é‡æ•°æ®å¯¼å…¥
- **æ‰¹é‡æ“ä½œ**ï¼šé«˜æ•ˆçš„æ‰¹é‡æ•°æ®åˆ›å»ºå’Œç®¡ç†

### ğŸ›¡ï¸ å®‰å…¨ä¿éšœ
- **ä»£ç å®¡æŸ¥**ï¼šASTé™æ€åˆ†æé˜²æ­¢æ¶æ„ä»£ç 
- **ç½‘ç»œå®‰å…¨**ï¼šURLç™½åå•å’Œå®‰å…¨æ£€æŸ¥
- **æ–‡ä»¶å®‰å…¨**ï¼šä¸¥æ ¼çš„æ–‡ä»¶ç±»å‹å’Œå¤§å°é™åˆ¶
- **æƒé™æ§åˆ¶**ï¼šåŸºäºè§’è‰²çš„è®¿é—®æ§åˆ¶

### ğŸ“ˆ æœ€ä½³å®è·µå»ºè®®
1. **éµå¾ªè§„èŒƒ**ï¼šä¸¥æ ¼æŒ‰ç…§æ’ä»¶æ¥å£è§„èŒƒå¼€å‘
2. **é”™è¯¯å¤„ç†**ï¼šå®ç°å®Œå–„çš„å¼‚å¸¸å¤„ç†æœºåˆ¶
3. **æ€§èƒ½ä¼˜åŒ–**ï¼šåˆç†ä½¿ç”¨ç¼“å­˜å’Œæ‰¹é‡å¤„ç†
4. **èµ„æºç®¡ç†**ï¼šåŠæ—¶æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç½‘ç»œè¿æ¥
5. **å®‰å…¨ç¼–ç **ï¼šé¿å…ä½¿ç”¨ç¦ç”¨çš„æ¨¡å—å’Œå‡½æ•°

### ğŸ”§ å¼€å‘å·¥å…·
- **é…ç½®ç³»ç»Ÿ**ï¼šæ”¯æŒå¤šç§å­—æ®µç±»å‹çš„åŠ¨æ€é…ç½®
- **æµ‹è¯•æ¥å£**ï¼šå†…ç½®çš„è¿æ¥æµ‹è¯•å’Œè°ƒè¯•åŠŸèƒ½
- **APIé›†æˆ**ï¼šå®Œæ•´çš„RESTful APIæ”¯æŒ
- **å‰ç«¯ç•Œé¢**ï¼šå¼€ç®±å³ç”¨çš„ç®¡ç†ç•Œé¢

è®°ä½å§‹ç»ˆéµå®ˆç›®æ ‡ç½‘ç«™çš„robots.txtå’ŒæœåŠ¡æ¡æ¬¾ï¼Œåˆç†æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œå°Šé‡æ•°æ®æºçš„ä½¿ç”¨é™åˆ¶ï¼Œåšä¸€ä¸ªè´Ÿè´£ä»»çš„æ•°æ®é‡‡é›†å¼€å‘è€…ã€‚

---

**å¼€å‘æ„‰å¿«ï¼** ğŸ‰

*å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ç³»ç»Ÿæ—¥å¿—ã€ä½¿ç”¨è°ƒè¯•æ¥å£ï¼Œæˆ–è”ç³»æŠ€æœ¯æ”¯æŒå›¢é˜Ÿã€‚*

### ğŸ“š ç›¸å…³èµ„æº

- **APIæ–‡æ¡£**ï¼š`/docs` - å®Œæ•´çš„APIæ¥å£æ–‡æ¡£
- **ç¤ºä¾‹ä»£ç **ï¼šå‚è€ƒæœ¬æ–‡æ¡£ä¸­çš„å®Œæ•´ç¤ºä¾‹
- **é”™è¯¯æ’æŸ¥**ï¼šæŸ¥çœ‹æ’ä»¶ç®¡ç†ç•Œé¢çš„è¯¦ç»†é”™è¯¯ä¿¡æ¯
- **æ€§èƒ½ç›‘æ§**ï¼šä½¿ç”¨ä»»åŠ¡ç®¡ç†ç•Œé¢è·Ÿè¸ªæ‰§è¡ŒçŠ¶æ€

### ğŸ†• ç‰ˆæœ¬æ›´æ–°

- **v2.1.0**ï¼šæ–°å¢å›¾ç‰‡ä¸‹è½½å’Œæ–‡ä»¶å¤„ç†åŠŸèƒ½
- **v2.0.0**ï¼šæ’ä»¶ç³»ç»Ÿé‡æ„ï¼Œå¢å¼ºå®‰å…¨æ€§
- **v1.x**ï¼šåŸºç¡€çˆ¬è™«æ’ä»¶ç³»ç»Ÿ